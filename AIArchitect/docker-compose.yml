# =============================================================================
# AI Architect - Trading Decision System
# Production Docker Compose - Full Setup with LLM Containers
# =============================================================================
#
# USAGE:
#   docker compose up -d
#
# PORTS:
#   8060 - Main LLM (Qwen2.5-32B)
#   8061 - Critic LLM (DeepSeek-R1-14B)
#   8062 - Embeddings (BGE-large)
#   8063 - Qdrant (Vector Database)
#   8064 - PostgreSQL (Trade Logs)
#   8065 - Backend Web UI
#
# GPU ALLOCATION (Dual GPU Setup):
#   GPU 0 (RTX 3090 24GB): Main LLM - Qwen 32B (~18GB)
#   GPU 1 (RTX 3080 Ti 12GB): Critic LLM (~8GB) + Embeddings (~2GB)
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # MAIN LLM - Qwen2.5-32B-Instruct-AWQ (requires ~18-20GB VRAM)
  # ---------------------------------------------------------------------------
  llm_main:
    image: vllm/vllm-openai:v0.6.3.post1
    container_name: llm_main_qwen32b
    environment:
      - CUDA_VISIBLE_DEVICES=0,1
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
    command: >
      --model Qwen/Qwen2.5-32B-Instruct-AWQ
      --quantization awq_marlin
      --dtype float16
      --max-model-len 1024
      --tensor-parallel-size 2
      --gpu-memory-utilization 0.92
      --trust-remote-code
      --enforce-eager
    ports:
      - "8060:8000"
    shm_size: '2gb'
    volumes:
      - huggingface_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # ---------------------------------------------------------------------------
  # CRITIC LLM - Qwen2.5-7B-Instruct-AWQ (fits in ~5GB)
  # ---------------------------------------------------------------------------
  llm_critic:
    image: vllm/vllm-openai:v0.6.3.post1
    container_name: llm_critic_qwen
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
    command: >
      --model Qwen/Qwen2.5-7B-Instruct-AWQ
      --quantization awq_marlin
      --dtype float16
      --max-model-len 4096
      --gpu-memory-utilization 0.70
      --trust-remote-code
    ports:
      - "8061:8000"
    volumes:
      - huggingface_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # ---------------------------------------------------------------------------
  # EMBEDDINGS - BGE-large-en-v1.5 (using Infinity server)
  # ---------------------------------------------------------------------------
  embeddings:
    image: michaelf34/infinity:0.0.51
    container_name: embeddings_bge
    restart: unless-stopped
    runtime: nvidia
    environment:
      - CUDA_VISIBLE_DEVICES=0
    command: v2 --model-id BAAI/bge-large-en-v1.5 --device cuda --port 80
    ports:
      - "8062:80"
    volumes:
      - huggingface_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]

  # ---------------------------------------------------------------------------
  # QDRANT - Vector Database for Semantic Memory
  # ---------------------------------------------------------------------------
  qdrant:
    image: qdrant/qdrant:latest
    container_name: aiarchitect_qdrant
    restart: unless-stopped
    ports:
      - "8063:6333"
    volumes:
      - qdrant_data:/qdrant/storage

  # ---------------------------------------------------------------------------
  # POSTGRESQL - Trade Logs & Statistics
  # ---------------------------------------------------------------------------
  postgres:
    image: postgres:16-alpine
    container_name: aiarchitect_postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: trades
      POSTGRES_USER: trader
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-traderpass}
    ports:
      - "8064:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro

  # ---------------------------------------------------------------------------
  # BACKEND - FastAPI Application with Web UI
  # ---------------------------------------------------------------------------
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: aiarchitect_backend
    restart: unless-stopped
    ports:
      - "8065:8065"
    environment:
      # LLM Configuration - Use vLLM containers or LocalAI
      - LLM_URL=${LLM_URL:-http://llm_main:8000}
      - LLM_MODEL=${LLM_MODEL:-Qwen/Qwen2.5-32B-Instruct-AWQ}
      - LLM_CRITIC_URL=http://llm_critic:8000
      - EMBEDDINGS_URL=http://embeddings:80
      # Database
      - DATABASE_URL=postgresql+asyncpg://trader:${POSTGRES_PASSWORD:-traderpass}@postgres:5432/trades
      # Qdrant
      - QDRANT_URL=http://qdrant:6333
    depends_on:
      - qdrant
      - postgres
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./rules:/app/rules

volumes:
  qdrant_data:
    name: aiarchitect_qdrant_data
  pg_data:
    name: aiarchitect_postgres_data
  huggingface_cache:
    name: aiarchitect_hf_cache
